{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62e45120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import pandas as pd, joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f77f88e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['LEFT','CENTRE','RIGHT']\n",
    "\n",
    "def _order_cols_3(est_classes):\n",
    "    est_classes = np.asarray(est_classes)\n",
    "    # numeric classes like [0,1,2]\n",
    "    if np.issubdtype(est_classes.dtype, np.integer):\n",
    "        pos = {int(c): j for j, c in enumerate(est_classes)}  # 0->col,1->col,2->col\n",
    "        return [pos[0], pos[1], pos[2]]\n",
    "    # string classes like ['LEFT','CENTRE','RIGHT'] or 'center'\n",
    "    canon = [str(c).upper().replace(\"CENTER\",\"CENTRE\") for c in est_classes]\n",
    "    pos = {c: j for j, c in enumerate(canon)}\n",
    "    return [pos['LEFT'], pos['CENTRE'], pos['RIGHT']]\n",
    "\n",
    "def get_oof_probas_3_fast(\n",
    "    estimator, X, y,\n",
    "    n_splits=3, random_state=42,\n",
    "    calib_cv=2, max_features_override=50000, verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Fast, leak-safe OOF probabilities for 3-class base models.\n",
    "    - y: array-like of ints in {0,1,2} (LEFT,CENTRE,RIGHT)\n",
    "    - Works for tabular or text Pipelines; calibrates if no predict_proba.\n",
    "    \"\"\"\n",
    "    y_arr = np.asarray(y, dtype=int).ravel()\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    oof = np.zeros((len(y_arr), 3), dtype=float)\n",
    "\n",
    "    for k, (tr, va) in enumerate(skf.split(np.zeros(len(y_arr)), y_arr), 1):\n",
    "        est = clone(estimator)\n",
    "\n",
    "        # Speed-up for text: cap TF-IDF features inside each fold if present\n",
    "        if isinstance(est, Pipeline) and 'tfidf' in est.named_steps and max_features_override is not None:\n",
    "            try:\n",
    "                est.named_steps['tfidf'].set_params(max_features=max_features_override)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Add calibration if the base lacks predict_proba (e.g., LinearSVC)\n",
    "        needs_cal = not hasattr(est, \"predict_proba\") and hasattr(est, \"decision_function\")\n",
    "        if needs_cal:\n",
    "            est = CalibratedClassifierCV(est, method='sigmoid', cv=calib_cv)\n",
    "\n",
    "        Xi_tr = X.iloc[tr] if hasattr(X, \"iloc\") else np.asarray(X)[tr]\n",
    "        Xi_va = X.iloc[va] if hasattr(X, \"iloc\") else np.asarray(X)[va]\n",
    "\n",
    "        est.fit(Xi_tr, y_arr[tr])\n",
    "        p = est.predict_proba(Xi_va)\n",
    "\n",
    "        order3 = _order_cols_3(getattr(est, \"classes_\", np.arange(p.shape[1])))\n",
    "        p3 = p[:, order3]\n",
    "\n",
    "        s = p3.sum(axis=1, keepdims=True); s[s==0] = 1.0\n",
    "        oof[va] = p3 / s\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Fold {k}/{n_splits} done (calibrated={needs_cal})\")\n",
    "\n",
    "    return oof\n",
    "\n",
    "def _entropy(a): a = np.clip(a, 1e-12, 1.0); return (-a*np.log(a)).sum(axis=1)\n",
    "def _margin(a):  t2 = np.sort(a, axis=1)[:, -2:]; return t2[:,1] - t2[:,0]\n",
    "\n",
    "def probs_to_df(p, prefix, n_rows):\n",
    "    if p is None:\n",
    "        df = pd.DataFrame({f'{prefix}_p_{c}': np.nan for c in CLASSES}, index=range(n_rows))\n",
    "        df[f'{prefix}_entropy'] = np.nan; df[f'{prefix}_margin'] = np.nan\n",
    "        return df\n",
    "    df = pd.DataFrame(p, columns=[f'{prefix}_p_{c}' for c in CLASSES])\n",
    "    df[f'{prefix}_entropy'] = _entropy(p)\n",
    "    df[f'{prefix}_margin']  = _margin(p)\n",
    "    return df\n",
    "\n",
    "def make_block(y, dataset_name, p_nela=None, p_factoid=None, p_anes=None):\n",
    "    n = len(y)\n",
    "    X = pd.concat([\n",
    "        probs_to_df(p_nela,    'nela',    n),\n",
    "        probs_to_df(p_factoid, 'factoid', n),\n",
    "        probs_to_df(p_anes,    'anes',    n),\n",
    "    ], axis=1)\n",
    "    X['dataset'] = dataset_name\n",
    "    return X, pd.Series(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1398cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_anes shape: (8277, 113)\n",
      "y_anes_3 counts:\n",
      " label\n",
      "CENTRE    3784\n",
      "LEFT      3363\n",
      "RIGHT     1130\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ROOT = Path.home() / \"Desktop\" / \"Dissertation\"\n",
    "ART  = ROOT / \"anes_artifacts\"\n",
    "\n",
    "pipeline_path = ART / \"anes_pipeline.joblib\"            # 3-class spec you saved\n",
    "labels_path   = ART / \"anes_train_labels_3.csv\"         # <-- 3-class\n",
    "parquet_path  = ART / \"anes_train_features.parquet\"\n",
    "csv_path      = ART / \"anes_train_features.csv\"\n",
    "\n",
    "# Load features (prefer Parquet, fall back to CSV)\n",
    "X_anes = pd.read_parquet(parquet_path) if parquet_path.exists() else pd.read_csv(csv_path)\n",
    "\n",
    "y_anes_3  = pd.read_csv(labels_path)['label'].astype(str)   # <-- 3-class labels\n",
    "model_anes = joblib.load(pipeline_path)\n",
    "\n",
    "print(\"X_anes shape:\", X_anes.shape)\n",
    "print(\"y_anes_3 counts:\\n\", y_anes_3.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9201b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_anes: (8277, 113) | unique y: [np.int64(0), np.int64(1), np.int64(2)] | counts: Counter({np.int64(1): 3784, np.int64(0): 3363, np.int64(2): 1130})\n",
      "Using n_splits_anes = 3\n"
     ]
    }
   ],
   "source": [
    "# Encode 3-class labels to 0..2 in fixed order\n",
    "ORDER3 = ['LEFT','CENTRE','RIGHT']\n",
    "y_anes_codes = pd.Categorical(\n",
    "    pd.Series(y_anes_3).astype(str).str.upper().str.replace(\"CENTER\",\"CENTRE\", regex=False),\n",
    "    categories=ORDER3, ordered=True\n",
    ").codes\n",
    "assert (y_anes_codes != -1).all(), \"Unmapped labels present in y_anes_3.\"\n",
    "\n",
    "# Align indices\n",
    "X_anes = X_anes.reset_index(drop=True)\n",
    "y_anes_codes = pd.Series(y_anes_codes).reset_index(drop=True).astype(int).values\n",
    "\n",
    "# Sanity checks\n",
    "assert len(X_anes) == len(y_anes_codes), f\"Length mismatch: X={len(X_anes)} vs y={len(y_anes_codes)}\"\n",
    "print(\"X_anes:\", X_anes.shape,\n",
    "      \"| unique y:\", sorted(np.unique(y_anes_codes)),\n",
    "      \"| counts:\", Counter(y_anes_codes))\n",
    "\n",
    "# Choose CV splits (≤ smallest class; min 2, cap at 3 for speed)\n",
    "cls_counts = pd.Series(y_anes_codes).value_counts()\n",
    "n_splits_anes = int(max(2, min(3, cls_counts.min())))\n",
    "print(\"Using n_splits_anes =\", n_splits_anes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "413e7759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/3 done (calibrated=False)\n",
      "Fold 2/3 done (calibrated=False)\n",
      "Fold 3/3 done (calibrated=False)\n",
      "anes_oof: (8277, 3) | row-sum min/max: 1.0 1.0\n",
      "Xa: (8277, 16)\n",
      "ya counts:\n",
      " label\n",
      "CENTRE    3784\n",
      "LEFT      3363\n",
      "RIGHT     1130\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# OOF (3-class) for ANES\n",
    "anes_oof = get_oof_probas_3_fast(\n",
    "    model_anes, X_anes, y_anes_codes,   # y_anes_codes from y_anes_3 → 0,1,2\n",
    "    n_splits=n_splits_anes, calib_cv=2, max_features_override=None, verbose=True\n",
    ")\n",
    "print(\"anes_oof:\", anes_oof.shape, \"| row-sum min/max:\",\n",
    "      np.round(anes_oof.sum(axis=1).min(),4), np.round(anes_oof.sum(axis=1).max(),4))\n",
    "\n",
    "# Build ANES meta-block (use 3-class labels)\n",
    "Xa, ya = make_block(\n",
    "    pd.Series(y_anes_3).str.upper().str.replace(\"CENTER\",\"CENTRE\", regex=False),\n",
    "    'anes',\n",
    "    p_anes=anes_oof\n",
    ")\n",
    "print(\"Xa:\", Xa.shape)\n",
    "print(\"ya counts:\\n\", ya.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32f67c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dissertation/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.0.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/dissertation/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.0.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/dissertation/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.0.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/dissertation/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.0.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded NELA: 1772948 texts\n",
      "NELA splits (fast): 3 | counts: {0: 765787, 1: 338588, 2: 668573}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'transform_input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m oof\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# 4) FAST OOF call\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m nela_oof \u001b[38;5;241m=\u001b[39m \u001b[43mget_oof_probas_any_fast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_nela\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnela_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_nela_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_splits_nela\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalib_cv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_features_override\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     95\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnela_oof:\u001b[39m\u001b[38;5;124m\"\u001b[39m, nela_oof\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| row-sum min/max:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     97\u001b[0m       np\u001b[38;5;241m.\u001b[39mround(nela_oof\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmin(),\u001b[38;5;241m4\u001b[39m), np\u001b[38;5;241m.\u001b[39mround(nela_oof\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmax(),\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# 5) Build NELA meta-block (define make_block fallback if missing)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 50\u001b[0m, in \u001b[0;36mget_oof_probas_any_fast\u001b[0;34m(estimator, X, y, n_splits, calib_cv, max_features_override, verbose)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, (tr, va) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(skf\u001b[38;5;241m.\u001b[39msplit(np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(y_arr)), y_arr), \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     49\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 50\u001b[0m     est \u001b[38;5;241m=\u001b[39m \u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# If Pipeline with 'tfidf', cap features for speed (only inside this fold)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(est, Pipeline) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m est\u001b[38;5;241m.\u001b[39mnamed_steps \u001b[38;5;129;01mand\u001b[39;00m max_features_override \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dissertation/lib/python3.9/site-packages/sklearn/base.py:94\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct a new unfitted estimator with the same parameters.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03mClone does a deep copy of the model in an estimator\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03mFalse\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_clone__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misclass(estimator):\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__sklearn_clone__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _clone_parametrized(estimator, safe\u001b[38;5;241m=\u001b[39msafe)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dissertation/lib/python3.9/site-packages/sklearn/base.py:300\u001b[0m, in \u001b[0;36mBaseEstimator.__sklearn_clone__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__sklearn_clone__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clone_parametrized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dissertation/lib/python3.9/site-packages/sklearn/base.py:125\u001b[0m, in \u001b[0;36m_clone_parametrized\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    118\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit does not seem to be a scikit-learn \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator as it does not implement a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mrepr\u001b[39m(estimator), \u001b[38;5;28mtype\u001b[39m(estimator))\n\u001b[1;32m    122\u001b[0m             )\n\u001b[1;32m    124\u001b[0m klass \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[0;32m--> 125\u001b[0m new_object_params \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m new_object_params\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    127\u001b[0m     new_object_params[name] \u001b[38;5;241m=\u001b[39m clone(param, safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dissertation/lib/python3.9/site-packages/sklearn/pipeline.py:299\u001b[0m, in \u001b[0;36mPipeline.get_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_params\u001b[39m(\u001b[38;5;28mself\u001b[39m, deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    283\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get parameters for this estimator.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m    Returns the parameters given in the constructor as well as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m        Parameter names mapped to their values.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dissertation/lib/python3.9/site-packages/sklearn/utils/metaestimators.py:30\u001b[0m, in \u001b[0;36m_BaseComposition._get_params\u001b[0;34m(self, attr, deep)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_params\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr, deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 30\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m deep:\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dissertation/lib/python3.9/site-packages/sklearn/base.py:248\u001b[0m, in \u001b[0;36mBaseEstimator.get_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    246\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_param_names():\n\u001b[0;32m--> 248\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m    250\u001b[0m         deep_items \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mget_params()\u001b[38;5;241m.\u001b[39mitems()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'transform_input'"
     ]
    }
   ],
   "source": [
    "# ===== N1 (FAST): Load NELA artifacts → fast OOF → meta block =====\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, joblib, time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "ROOT  = Path.home() / \"Desktop\" / \"Dissertation\"\n",
    "ARTN  = ROOT / \"nela_artifacts\"\n",
    "\n",
    "# 1) Load artifacts\n",
    "nela_texts = pd.read_csv(ARTN / \"nela_train_text.csv\")[\"text\"].astype(str).fillna(\"\")\n",
    "y_nela_3   = pd.read_csv(ARTN / \"nela_train_labels_3.csv\")[\"label\"].astype(str)\n",
    "model_nela = joblib.load(ARTN / \"nela_pipeline.joblib\")\n",
    "print(\"Loaded NELA:\", len(nela_texts), \"texts\")\n",
    "\n",
    "# 2) Encode labels to 0..2 (LEFT,CENTRE,RIGHT)\n",
    "ORDER3 = ['LEFT','CENTRE','RIGHT']\n",
    "y_nela_codes = pd.Categorical(\n",
    "    y_nela_3.str.upper().str.replace(\"CENTER\",\"CENTRE\", regex=False),\n",
    "    categories=ORDER3, ordered=True\n",
    ").codes\n",
    "\n",
    "# 3) Pick fast CV folds (≤ smallest class, cap at 3; min 2)\n",
    "cls_counts = pd.Series(y_nela_codes).value_counts()\n",
    "n_splits_nela = int(max(2, min(3, cls_counts.min())))\n",
    "print(\"NELA splits (fast):\", n_splits_nela, \"| counts:\", cls_counts.sort_index().to_dict())\n",
    "\n",
    "def _order_cols_3(est_classes):\n",
    "    est_classes = np.array(est_classes)\n",
    "    if np.issubdtype(est_classes.dtype, np.integer):\n",
    "        m = {int(c): j for j, c in enumerate(est_classes)}  # [0,1,2]\n",
    "        return [m[0], m[1], m[2]]\n",
    "    canon = [str(c).upper().replace(\"CENTER\",\"CENTRE\") for c in est_classes]\n",
    "    m = {c: j for j, c in enumerate(canon)}\n",
    "    return [m['LEFT'], m['CENTRE'], m['RIGHT']]\n",
    "\n",
    "def get_oof_probas_any_fast(\n",
    "    estimator, X, y,\n",
    "    n_splits=3, calib_cv=2, max_features_override=50000, verbose=True\n",
    "):\n",
    "    \"\"\"Fast OOF: fewer folds, lighter calibration, cap TF-IDF max_features during OOF.\"\"\"\n",
    "    y_arr = np.asarray(y, dtype=int).ravel()\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    oof = np.zeros((len(y_arr), 3), dtype=float)\n",
    "\n",
    "    for fold, (tr, va) in enumerate(skf.split(np.zeros(len(y_arr)), y_arr), 1):\n",
    "        t0 = time.time()\n",
    "        est = clone(estimator)\n",
    "\n",
    "        # If Pipeline with 'tfidf', cap features for speed (only inside this fold)\n",
    "        if isinstance(est, Pipeline) and 'tfidf' in est.named_steps and max_features_override is not None:\n",
    "            try:\n",
    "                est.named_steps['tfidf'].set_params(max_features=max_features_override)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Wrap with calibrator if no predict_proba but has decision_function (e.g., LinearSVC)\n",
    "        needs_cal = not hasattr(est, \"predict_proba\") and hasattr(est, \"decision_function\")\n",
    "        if needs_cal:\n",
    "            est = CalibratedClassifierCV(est, method='sigmoid', cv=calib_cv)\n",
    "\n",
    "        Xi_tr = X.iloc[tr] if hasattr(X, \"iloc\") else np.asarray(X)[tr]\n",
    "        Xi_va = X.iloc[va] if hasattr(X, \"iloc\") else np.asarray(X)[va]\n",
    "\n",
    "        est.fit(Xi_tr, y_arr[tr])\n",
    "        p = est.predict_proba(Xi_va)\n",
    "        est_classes = getattr(est, \"classes_\", np.arange(p.shape[1]))\n",
    "\n",
    "        if p.shape[1] == 5:\n",
    "            # Ensure [EL, L, C, R, ER] then collapse to 3\n",
    "            if np.issubdtype(np.asarray(est_classes).dtype, np.integer):\n",
    "                order5 = np.argsort(est_classes)  # 0..4\n",
    "                p5 = p[:, order5]\n",
    "            else:\n",
    "                canon = [str(c).upper().replace(' ', '_') for c in est_classes]\n",
    "                order5 = [canon.index(k) for k in ['EXTREME_LEFT','LEFT','CENTRE','RIGHT','EXTREME_RIGHT']]\n",
    "                p5 = p[:, order5]\n",
    "            p3 = np.stack([p5[:,0]+p5[:,1], p5[:,2], p5[:,3]+p5[:,4]], axis=1)\n",
    "        else:\n",
    "            p3 = p[:, _order_cols_3(est_classes)]\n",
    "\n",
    "        s = p3.sum(axis=1, keepdims=True); s[s==0] = 1.0\n",
    "        oof[va] = p3 / s\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Fold {fold}/{n_splits} done in {time.time()-t0:.1f}s (calibrated={needs_cal})\")\n",
    "    return oof\n",
    "\n",
    "# 4) FAST OOF call\n",
    "nela_oof = get_oof_probas_any_fast(\n",
    "    model_nela, nela_texts, y_nela_codes,\n",
    "    n_splits=n_splits_nela, calib_cv=2, max_features_override=50000, verbose=True\n",
    ")\n",
    "print(\"nela_oof:\", nela_oof.shape, \"| row-sum min/max:\",\n",
    "      np.round(nela_oof.sum(axis=1).min(),4), np.round(nela_oof.sum(axis=1).max(),4))\n",
    "\n",
    "# 5) Build NELA meta-block (define make_block fallback if missing)\n",
    "if 'make_block' not in globals():\n",
    "    def _entropy(a): a = np.clip(a, 1e-12, 1.0); return (-a*np.log(a)).sum(axis=1)\n",
    "    def _margin(a): t2 = np.sort(a, axis=1)[:, -2:]; return t2[:,1]-t2[:,0]\n",
    "    def probs_to_df(p, prefix, n):\n",
    "        CLASSES = ['LEFT','CENTRE','RIGHT']\n",
    "        df = pd.DataFrame(p, columns=[f'{prefix}_p_{c}' for c in CLASSES])\n",
    "        df[f'{prefix}_entropy'] = _entropy(p); df[f'{prefix}_margin'] = _margin(p)\n",
    "        return df\n",
    "    def make_block(y, dataset_name, p_nela=None, p_factoid=None, p_anes=None):\n",
    "        n = len(y)\n",
    "        CLASSES = ['LEFT','CENTRE','RIGHT']\n",
    "        frames = []\n",
    "        for pfx, p in [('nela', p_nela), ('factoid', p_factoid), ('anes', p_anes)]:\n",
    "            if p is None:\n",
    "                df = pd.DataFrame({f'{pfx}_p_{c}': np.nan for c in CLASSES}, index=range(n))\n",
    "                df[f'{pfx}_entropy'] = np.nan; df[f'{pfx}_margin'] = np.nan\n",
    "            else:\n",
    "                df = probs_to_df(p, pfx, n)\n",
    "            frames.append(df)\n",
    "        X = pd.concat(frames, axis=1); X['dataset'] = dataset_name\n",
    "        return X, pd.Series(y)\n",
    "\n",
    "Xn, yn = make_block(\n",
    "    y_nela_3.str.upper().str.replace(\"CENTER\",\"CENTRE\", regex=False),\n",
    "    'nela',\n",
    "    p_nela=nela_oof\n",
    ")\n",
    "print(\"Xn:\", Xn.shape)\n",
    "print(\"yn counts:\\n\", yn.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1278cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dissertation/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.0.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/dissertation/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.0.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/dissertation/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.0.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/dissertation/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.0.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded NELA: 1772948 texts\n",
      "NELA splits (fast): 3 | counts: {0: 765787, 1: 338588, 2: 668573}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'transform_input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m oof\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# 4) FAST OOF call\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m nela_oof \u001b[38;5;241m=\u001b[39m \u001b[43mget_oof_probas_any_fast\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_nela\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnela_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_nela_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_splits_nela\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalib_cv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_features_override\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     95\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnela_oof:\u001b[39m\u001b[38;5;124m\"\u001b[39m, nela_oof\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| row-sum min/max:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     97\u001b[0m       np\u001b[38;5;241m.\u001b[39mround(nela_oof\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmin(),\u001b[38;5;241m4\u001b[39m), np\u001b[38;5;241m.\u001b[39mround(nela_oof\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmax(),\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# 5) Build NELA meta-block (define make_block fallback if missing)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 50\u001b[0m, in \u001b[0;36mget_oof_probas_any_fast\u001b[0;34m(estimator, X, y, n_splits, calib_cv, max_features_override, verbose)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, (tr, va) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(skf\u001b[38;5;241m.\u001b[39msplit(np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(y_arr)), y_arr), \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     49\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 50\u001b[0m     est \u001b[38;5;241m=\u001b[39m \u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# If Pipeline with 'tfidf', cap features for speed (only inside this fold)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(est, Pipeline) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m est\u001b[38;5;241m.\u001b[39mnamed_steps \u001b[38;5;129;01mand\u001b[39;00m max_features_override \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dissertation/lib/python3.9/site-packages/sklearn/base.py:94\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct a new unfitted estimator with the same parameters.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03mClone does a deep copy of the model in an estimator\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03mFalse\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_clone__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misclass(estimator):\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__sklearn_clone__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _clone_parametrized(estimator, safe\u001b[38;5;241m=\u001b[39msafe)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dissertation/lib/python3.9/site-packages/sklearn/base.py:300\u001b[0m, in \u001b[0;36mBaseEstimator.__sklearn_clone__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__sklearn_clone__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_clone_parametrized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dissertation/lib/python3.9/site-packages/sklearn/base.py:125\u001b[0m, in \u001b[0;36m_clone_parametrized\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    118\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit does not seem to be a scikit-learn \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator as it does not implement a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mrepr\u001b[39m(estimator), \u001b[38;5;28mtype\u001b[39m(estimator))\n\u001b[1;32m    122\u001b[0m             )\n\u001b[1;32m    124\u001b[0m klass \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[0;32m--> 125\u001b[0m new_object_params \u001b[38;5;241m=\u001b[39m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m new_object_params\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    127\u001b[0m     new_object_params[name] \u001b[38;5;241m=\u001b[39m clone(param, safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dissertation/lib/python3.9/site-packages/sklearn/pipeline.py:299\u001b[0m, in \u001b[0;36mPipeline.get_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_params\u001b[39m(\u001b[38;5;28mself\u001b[39m, deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    283\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get parameters for this estimator.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m    Returns the parameters given in the constructor as well as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m        Parameter names mapped to their values.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dissertation/lib/python3.9/site-packages/sklearn/utils/metaestimators.py:30\u001b[0m, in \u001b[0;36m_BaseComposition._get_params\u001b[0;34m(self, attr, deep)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_params\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr, deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 30\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m deep:\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dissertation/lib/python3.9/site-packages/sklearn/base.py:248\u001b[0m, in \u001b[0;36mBaseEstimator.get_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    246\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_param_names():\n\u001b[0;32m--> 248\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m    250\u001b[0m         deep_items \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mget_params()\u001b[38;5;241m.\u001b[39mitems()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'transform_input'"
     ]
    }
   ],
   "source": [
    "# ===== N1 (FAST): Load NELA artifacts → fast OOF → meta block =====\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, joblib, time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "ROOT  = Path.home() / \"Desktop\" / \"Dissertation\"\n",
    "ARTN  = ROOT / \"nela_artifacts\"\n",
    "\n",
    "# 1) Load artifacts\n",
    "nela_texts = pd.read_csv(ARTN / \"nela_train_text.csv\")[\"text\"].astype(str).fillna(\"\")\n",
    "y_nela_3   = pd.read_csv(ARTN / \"nela_train_labels_3.csv\")[\"label\"].astype(str)\n",
    "model_nela = joblib.load(ARTN / \"nela_pipeline.joblib\")\n",
    "print(\"Loaded NELA:\", len(nela_texts), \"texts\")\n",
    "\n",
    "# 2) Encode labels to 0..2 (LEFT,CENTRE,RIGHT)\n",
    "ORDER3 = ['LEFT','CENTRE','RIGHT']\n",
    "y_nela_codes = pd.Categorical(\n",
    "    y_nela_3.str.upper().str.replace(\"CENTER\",\"CENTRE\", regex=False),\n",
    "    categories=ORDER3, ordered=True\n",
    ").codes\n",
    "\n",
    "# 3) Pick fast CV folds (≤ smallest class, cap at 3; min 2)\n",
    "cls_counts = pd.Series(y_nela_codes).value_counts()\n",
    "n_splits_nela = int(max(2, min(3, cls_counts.min())))\n",
    "print(\"NELA splits (fast):\", n_splits_nela, \"| counts:\", cls_counts.sort_index().to_dict())\n",
    "\n",
    "def _order_cols_3(est_classes):\n",
    "    est_classes = np.array(est_classes)\n",
    "    if np.issubdtype(est_classes.dtype, np.integer):\n",
    "        m = {int(c): j for j, c in enumerate(est_classes)}  # [0,1,2]\n",
    "        return [m[0], m[1], m[2]]\n",
    "    canon = [str(c).upper().replace(\"CENTER\",\"CENTRE\") for c in est_classes]\n",
    "    m = {c: j for j, c in enumerate(canon)}\n",
    "    return [m['LEFT'], m['CENTRE'], m['RIGHT']]\n",
    "\n",
    "def get_oof_probas_any_fast(\n",
    "    estimator, X, y,\n",
    "    n_splits=3, calib_cv=2, max_features_override=50000, verbose=True\n",
    "):\n",
    "    \"\"\"Fast OOF: fewer folds, lighter calibration, cap TF-IDF max_features during OOF.\"\"\"\n",
    "    y_arr = np.asarray(y, dtype=int).ravel()\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    oof = np.zeros((len(y_arr), 3), dtype=float)\n",
    "\n",
    "    for fold, (tr, va) in enumerate(skf.split(np.zeros(len(y_arr)), y_arr), 1):\n",
    "        t0 = time.time()\n",
    "        est = clone(estimator)\n",
    "\n",
    "        # If Pipeline with 'tfidf', cap features for speed (only inside this fold)\n",
    "        if isinstance(est, Pipeline) and 'tfidf' in est.named_steps and max_features_override is not None:\n",
    "            try:\n",
    "                est.named_steps['tfidf'].set_params(max_features=max_features_override)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Wrap with calibrator if no predict_proba but has decision_function (e.g., LinearSVC)\n",
    "        needs_cal = not hasattr(est, \"predict_proba\") and hasattr(est, \"decision_function\")\n",
    "        if needs_cal:\n",
    "            est = CalibratedClassifierCV(est, method='sigmoid', cv=calib_cv)\n",
    "\n",
    "        Xi_tr = X.iloc[tr] if hasattr(X, \"iloc\") else np.asarray(X)[tr]\n",
    "        Xi_va = X.iloc[va] if hasattr(X, \"iloc\") else np.asarray(X)[va]\n",
    "\n",
    "        est.fit(Xi_tr, y_arr[tr])\n",
    "        p = est.predict_proba(Xi_va)\n",
    "        est_classes = getattr(est, \"classes_\", np.arange(p.shape[1]))\n",
    "\n",
    "        if p.shape[1] == 5:\n",
    "            # Ensure [EL, L, C, R, ER] then collapse to 3\n",
    "            if np.issubdtype(np.asarray(est_classes).dtype, np.integer):\n",
    "                order5 = np.argsort(est_classes)  # 0..4\n",
    "                p5 = p[:, order5]\n",
    "            else:\n",
    "                canon = [str(c).upper().replace(' ', '_') for c in est_classes]\n",
    "                order5 = [canon.index(k) for k in ['EXTREME_LEFT','LEFT','CENTRE','RIGHT','EXTREME_RIGHT']]\n",
    "                p5 = p[:, order5]\n",
    "            p3 = np.stack([p5[:,0]+p5[:,1], p5[:,2], p5[:,3]+p5[:,4]], axis=1)\n",
    "        else:\n",
    "            p3 = p[:, _order_cols_3(est_classes)]\n",
    "\n",
    "        s = p3.sum(axis=1, keepdims=True); s[s==0] = 1.0\n",
    "        oof[va] = p3 / s\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Fold {fold}/{n_splits} done in {time.time()-t0:.1f}s (calibrated={needs_cal})\")\n",
    "    return oof\n",
    "\n",
    "# 4) FAST OOF call\n",
    "nela_oof = get_oof_probas_any_fast(\n",
    "    model_nela, nela_texts, y_nela_codes,\n",
    "    n_splits=n_splits_nela, calib_cv=2, max_features_override=50000, verbose=True\n",
    ")\n",
    "print(\"nela_oof:\", nela_oof.shape, \"| row-sum min/max:\",\n",
    "      np.round(nela_oof.sum(axis=1).min(),4), np.round(nela_oof.sum(axis=1).max(),4))\n",
    "\n",
    "# 5) Build NELA meta-block (define make_block fallback if missing)\n",
    "if 'make_block' not in globals():\n",
    "    def _entropy(a): a = np.clip(a, 1e-12, 1.0); return (-a*np.log(a)).sum(axis=1)\n",
    "    def _margin(a): t2 = np.sort(a, axis=1)[:, -2:]; return t2[:,1]-t2[:,0]\n",
    "    def probs_to_df(p, prefix, n):\n",
    "        CLASSES = ['LEFT','CENTRE','RIGHT']\n",
    "        df = pd.DataFrame(p, columns=[f'{prefix}_p_{c}' for c in CLASSES])\n",
    "        df[f'{prefix}_entropy'] = _entropy(p); df[f'{prefix}_margin'] = _margin(p)\n",
    "        return df\n",
    "    def make_block(y, dataset_name, p_nela=None, p_factoid=None, p_anes=None):\n",
    "        n = len(y)\n",
    "        CLASSES = ['LEFT','CENTRE','RIGHT']\n",
    "        frames = []\n",
    "        for pfx, p in [('nela', p_nela), ('factoid', p_factoid), ('anes', p_anes)]:\n",
    "            if p is None:\n",
    "                df = pd.DataFrame({f'{pfx}_p_{c}': np.nan for c in CLASSES}, index=range(n))\n",
    "                df[f'{pfx}_entropy'] = np.nan; df[f'{pfx}_margin'] = np.nan\n",
    "            else:\n",
    "                df = probs_to_df(p, pfx, n)\n",
    "            frames.append(df)\n",
    "        X = pd.concat(frames, axis=1); X['dataset'] = dataset_name\n",
    "        return X, pd.Series(y)\n",
    "\n",
    "Xn, yn = make_block(\n",
    "    y_nela_3.str.upper().str.replace(\"CENTER\",\"CENTRE\", regex=False),\n",
    "    'nela',\n",
    "    p_nela=nela_oof\n",
    ")\n",
    "print(\"Xn:\", Xn.shape)\n",
    "print(\"yn counts:\\n\", yn.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed21ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/env1/lib/python3.9/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.0.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/env1/lib/python3.9/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.0.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FACTOID splits: 5 | counts: {0: 2989, 1: 617, 2: 295}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/env1/lib/python3.9/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.0.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/env1/lib/python3.9/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.0.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/env1/lib/python3.9/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.0.1 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_oof_probas_any_fast' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFACTOID splits:\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_splits_fact, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| counts:\u001b[39m\u001b[38;5;124m\"\u001b[39m, cls_counts\u001b[38;5;241m.\u001b[39msort_index()\u001b[38;5;241m.\u001b[39mto_dict())\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# ==== FACTOID: BUILD OOF (3-class) ====\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m factoid_oof \u001b[38;5;241m=\u001b[39m \u001b[43mget_oof_probas_any_fast\u001b[49m(model_factoid, fact_texts, y_fact_codes, n_splits\u001b[38;5;241m=\u001b[39mn_splits_fact)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfactoid_oof:\u001b[39m\u001b[38;5;124m\"\u001b[39m, factoid_oof\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| row-sum min/max:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m       np\u001b[38;5;241m.\u001b[39mround(factoid_oof\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmin(),\u001b[38;5;241m4\u001b[39m), np\u001b[38;5;241m.\u001b[39mround(factoid_oof\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmax(),\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# ==== FACTOID: BUILD META-BLOCK ====\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_oof_probas_any_fast' is not defined"
     ]
    }
   ],
   "source": [
    "# ==== FACTOID: LOAD ARTIFACTS ====\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, joblib\n",
    "\n",
    "ROOT = Path.home() / \"Desktop\" / \"Dissertation\"\n",
    "ARTF = ROOT / \"factoid_artifacts\"\n",
    "\n",
    "fact_texts   = pd.read_csv(ARTF / \"factoid_train_text.csv\")[\"text\"]\n",
    "y_fact_3     = pd.read_csv(ARTF / \"factoid_train_labels_3.csv\")[\"label\"].astype(str)\n",
    "model_factoid = joblib.load(ARTF / \"factoid_pipeline.joblib\")\n",
    "\n",
    "# Encode labels to 0..2 in fixed order\n",
    "ORDER3 = ['LEFT','CENTRE','RIGHT']\n",
    "y_fact_codes = pd.Categorical(y_fact_3.str.upper(), categories=ORDER3, ordered=True).codes\n",
    "\n",
    "# Choose CV splits safely (can’t exceed smallest class size)\n",
    "cls_counts = pd.Series(y_fact_codes).value_counts()\n",
    "n_splits_fact = int(max(2, min(5, cls_counts.min())))\n",
    "print(\"FACTOID splits:\", n_splits_fact, \"| counts:\", cls_counts.sort_index().to_dict())\n",
    "\n",
    "# ==== FACTOID: BUILD OOF (3-class) ====\n",
    "factoid_oof = get_oof_probas_any_fast(model_factoid, fact_texts, y_fact_codes, n_splits=n_splits_fact)\n",
    "print(\"factoid_oof:\", factoid_oof.shape, \"| row-sum min/max:\",\n",
    "      np.round(factoid_oof.sum(axis=1).min(),4), np.round(factoid_oof.sum(axis=1).max(),4))\n",
    "\n",
    "# ==== FACTOID: BUILD META-BLOCK ====\n",
    "Xf, yf = make_block(y_fact_3.str.upper().str.replace(\"CENTER\",\"CENTRE\", regex=False), 'factoid',\n",
    "                    p_factoid=factoid_oof)\n",
    "\n",
    "print(\"Xf:\", Xf.shape)\n",
    "print(\"yf counts:\\n\", yf.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f185efb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     10\u001b[0m blocks \u001b[38;5;241m=\u001b[39m [blk \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m [Xn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXn\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m                           Xf \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXf\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m                           Xa \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXa\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;28;01mif\u001b[39;00m blk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     13\u001b[0m labels \u001b[38;5;241m=\u001b[39m [lbl \u001b[38;5;28;01mfor\u001b[39;00m lbl \u001b[38;5;129;01min\u001b[39;00m [yn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myn\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m                           yf \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myf\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m                           ya \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mya\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;28;01mif\u001b[39;00m lbl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m---> 17\u001b[0m X_meta \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m y_meta \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(labels, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncluded datasets:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mto_dict())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env1/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env1/lib/python3.9/site-packages/pandas/core/reshape/concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconcat\u001b[39m(\n\u001b[1;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env1/lib/python3.9/site-packages/pandas/core/reshape/concat.py:425\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    422\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# === N2 (ALL-THREE): train meta on Xn+Xf+Xa ===\n",
    "import pandas as pd, numpy as np, joblib\n",
    "from pathlib import Path\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Concatenate available blocks\n",
    "blocks = [blk for blk in [Xn if 'Xn' in globals() else None,\n",
    "                          Xf if 'Xf' in globals() else None,\n",
    "                          Xa if 'Xa' in globals() else None] if blk is not None]\n",
    "labels = [lbl for lbl in [yn if 'yn' in globals() else None,\n",
    "                          yf if 'yf' in globals() else None,\n",
    "                          ya if 'ya' in globals() else None] if lbl is not None]\n",
    "\n",
    "X_meta = pd.concat(blocks, axis=0, ignore_index=True)\n",
    "y_meta = pd.concat(labels, axis=0, ignore_index=True)\n",
    "print(\"Included datasets:\", X_meta['dataset'].value_counts().to_dict())\n",
    "\n",
    "# One-hot dataset tag\n",
    "X_meta = pd.get_dummies(X_meta, columns=['dataset'], drop_first=False)\n",
    "META_COLUMNS = X_meta.columns.tolist()\n",
    "\n",
    "# Encode labels\n",
    "CLASSES = ['LEFT','CENTRE','RIGHT']\n",
    "le = LabelEncoder().fit(CLASSES)\n",
    "y_enc = le.transform(y_meta)\n",
    "\n",
    "# Optional class weights\n",
    "cls_counts = pd.Series(y_enc).value_counts()\n",
    "cls_weight = {i: (len(y_enc)/(len(cls_counts)*cls_counts[i])) for i in cls_counts.index}\n",
    "w = pd.Series(y_enc).map(cls_weight).values\n",
    "\n",
    "# Train/val split + train meta\n",
    "X_tr, X_va, y_tr, y_va, w_tr, w_va = train_test_split(\n",
    "    X_meta, y_enc, w, test_size=0.2, stratify=y_enc, random_state=42\n",
    ")\n",
    "meta = XGBClassifier(\n",
    "    objective='multi:softprob', num_class=3,\n",
    "    n_estimators=700, learning_rate=0.035, max_depth=4,\n",
    "    subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
    "    tree_method='hist', eval_metric='mlogloss', n_jobs=-1\n",
    ")\n",
    "meta.fit(X_tr, y_tr, sample_weight=w_tr)\n",
    "\n",
    "# Eval + save\n",
    "pred = meta.predict(X_va)\n",
    "pred_lbl, true_lbl = le.inverse_transform(pred), le.inverse_transform(y_va)\n",
    "print(\"Meta Accuracy:\", accuracy_score(true_lbl, pred_lbl))\n",
    "print(classification_report(true_lbl, pred_lbl, digits=3, labels=CLASSES))\n",
    "print(pd.DataFrame(confusion_matrix(true_lbl, pred_lbl, labels=CLASSES),\n",
    "                   index=CLASSES, columns=CLASSES))\n",
    "\n",
    "META_DIR = Path.home() / \"Desktop\" / \"Dissertation\" / \"meta_artifacts\"\n",
    "META_DIR.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(meta, META_DIR / \"meta_model.joblib\")\n",
    "joblib.dump(META_COLUMNS, META_DIR / \"meta_columns.joblib\")\n",
    "joblib.dump(le, META_DIR / \"meta_label_encoder.joblib\")\n",
    "print(\"Saved:\", [p.name for p in META_DIR.iterdir()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf901a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix, roc_auc_score, average_precision_score, brier_score_loss\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# meta_df must have at least: ['dataset','y_true_LCR','p_left','p_centre','p_right']\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m fact \u001b[38;5;241m=\u001b[39m \u001b[43mmeta\u001b[49m[meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFACTOID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 1) score\u001b[39;00m\n\u001b[1;32m      8\u001b[0m fact[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms_meta\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m fact[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp_centre\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'meta' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "# meta_df must have at least: ['dataset','y_true_LCR','p_left','p_centre','p_right']\n",
    "fact = meta[meta['dataset']=='FACTOID'].copy()\n",
    "\n",
    "# 1) score\n",
    "fact['s_meta'] = 1 - fact['p_centre']\n",
    "\n",
    "# 2) true binary\n",
    "y_true_bin = (fact['y_true_LCR'] != 'CENTRE').astype(int).to_numpy()\n",
    "\n",
    "# 3) pick a threshold (reuse 0.56 to mirror your standalone, or tune on a FACTOID val split)\n",
    "t = 0.56\n",
    "y_hat_bin = (fact['s_meta'] >= t).astype(int)\n",
    "\n",
    "# 4) metrics\n",
    "print(classification_report(y_true_bin, y_hat_bin, target_names=['CENTRE','POLARIZED']))\n",
    "print(\"AUROC:\", roc_auc_score(y_true_bin, fact['s_meta']))\n",
    "print(\"AP:\", average_precision_score(y_true_bin, fact['s_meta']))\n",
    "print(\"Brier:\", brier_score_loss(y_true_bin, fact['s_meta']))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_true_bin, y_hat_bin))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
